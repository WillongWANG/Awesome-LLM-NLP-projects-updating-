{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "# from thuglm.modeling_chatglm import ChatGLMForConditionalGeneration\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e9863bc3d8418b9fb834163d3bc62d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/peft/tuners/lora.py:173: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): ChatGLMForConditionalGeneration(\n",
       "      (transformer): ChatGLMModel(\n",
       "        (word_embeddings): Embedding(130528, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x GLMBlock(\n",
       "            (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SelfAttention(\n",
       "              (rotary_emb): RotaryEmbedding()\n",
       "              (query_key_value): MergedLinear(\n",
       "                in_features=4096, out_features=12288, bias=True\n",
       "                (lora_dropout): Dropout(p=0.1, inplace=False)\n",
       "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                (lora_B): Conv1d(16, 8192, kernel_size=(1,), stride=(1,), groups=2, bias=False)\n",
       "              )\n",
       "              (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GLU(\n",
       "              (dense_h_to_4h): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "              (dense_4h_to_h): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    \"/root/chatglm6b-local/models--THUDM--chatglm-6b/snapshots/bf0f5cfb575eebebf9b655c5861177acfee03f16\", trust_remote_code=True).half().cuda()\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=['query_key_value',], # 见下ChatGLMModel...\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 在这里加载lora模型，注意修改chekpoint\n",
    "peft_path = \"/root/zero_nlp/simple_thu_chatglm6b/test004/chatglm-lora.pt\"\n",
    "model.load_state_dict(torch.load(peft_path,map_location='cuda'), strict=False)\n",
    "model = model.half().cuda() # important!\n",
    "for name, param in model.named_parameters():\n",
    "    if param.device != torch.device('cuda:0'):\n",
    "        print(f\"Warning: Parameter {name} is on {param.device}\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"/root/chatglm6b-local/models--THUDM--chatglm-6b/snapshots/bf0f5cfb575eebebf9b655c5861177acfee03f16\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     5,  65421,     61,  75898,     32,  68554,     61,  77257,  64555,\n",
      "             32,  65107,     61,  66268,     32,  65347,     61,  71689,     32,\n",
      "          69768,     61,  85428,     32,  65173,  73942,     61,  70984,     32,\n",
      "          65173,  70936,     61,  64703,  65509, 130001, 130004]],\n",
      "       device='cuda:0')}\n",
      "类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞 这款上衣采用了简约的牛仔布材质,白色刺绣图案点缀其中,增添了不少时尚感。衣款式采用了破洞设计,个性十足。\n"
     ]
    }
   ],
   "source": [
    "# text =\"为什么冰红茶和柠檬茶的味道一样？\"\n",
    "\n",
    "# with torch.autocast(\"cuda\"):\n",
    "#     res, history = model.chat(tokenizer=tokenizer, query=text,max_length=300)\n",
    "#     print(res)\n",
    "\n",
    "text = \"类型#上衣*材质#牛仔布*颜色#白色*风格#简约*图案#刺绣*衣样式#外套*衣款式#破洞\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Convert attention mask to bool type explicitly\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
    "    print(inputs)\n",
    "    \n",
    "    # Generate using the base model's generate method directly\n",
    "    generate_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100\n",
    "    )\n",
    "    \n",
    "    # Decode output\n",
    "    output = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
